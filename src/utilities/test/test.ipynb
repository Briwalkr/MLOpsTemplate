{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 7.0.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('pyarrow<4.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "kv=ws.get_default_keyvault()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisite\n",
    "If you use the provision feature, then just need a workspace ws object\n",
    "If you need to create the cluster manually\n",
    "    1. Create a service principal and secret (SP)\n",
    "    2. Provision an ADX cluster and create a DB\n",
    "    3. Add the SP to be contributor of the cluster\n",
    "    4. pip install following packages: pip install --upgrade azure-mgmt-kusto azure-kusto-ingest azure-kusto-data azure-identity \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provisioning resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provision ADX cluster and register neccessary information at the workspace's keyvault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option to set parameters to a custom ADX cluster\n",
    "from monitoring import KV_SP_ID, KV_SP_KEY, KV_ADX_DB, KV_ADX_URI, KV_TENANT_ID\n",
    "tenant_id = \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
    "#Application ID\n",
    "client_id = \"af883abf-89dd-4889-bdb3-1ee84f68465e\"\n",
    "#Client Secret, set it at your WS' keyvault with key name same as your client_id\n",
    "client_secret = kv.get_secret(client_id)\n",
    "subscription_id = \"0e9bace8-7a81-4922-83b5-d995ff706507\"\n",
    "\n",
    "cluster_uri = \"https://adx02.westus2.kusto.windows.net\" #URL of the ADX Cluster\n",
    "kv.set_secret(KV_SP_ID,client_id)\n",
    "kv.set_secret(KV_SP_KEY,client_secret)\n",
    "kv.set_secret(KV_ADX_DB,\"db01\")\n",
    "kv.set_secret(KV_ADX_URI,cluster_uri)\n",
    "kv.set_secret(KV_TENANT_ID,tenant_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Service Principal\n",
      "begin creating ADX cluster ws01entmonitoring349 at westus2 with Dev(No SLA)_Standard_D11_v2 and capacity 1\n",
      "finished creating cluster ws01entmonitoring349\n",
      "begin creating DB mlmonitoring for cluster ws01entmonitoring349\n",
      "finished creating database\n"
     ]
    }
   ],
   "source": [
    "from monitoring.management import provision\n",
    "provision(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.data_collector import Online_Collector\n",
    "table_name = \"isd_weather_test4\" #new dataset\n",
    "\n",
    "sample_pd_data = pd.read_parquet(\"data/test_data.parquet\").head(10)\n",
    "sample_pd_data['timestamp'] = sample_pd_data['datetime']\n",
    "sample_pd_data.drop(['datetime'], inplace=True, axis=1)\n",
    "\n",
    "online_collector = Online_Collector(table_name,ws=ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.cluster_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.stream_collect_df(sample_pd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.batch_collect(sample_pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Ingestion and Real Time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ingest streaming data  asynchronously with internal buffering mechanism to lower impact to main scoring thread\n",
    "streaming_table_name=\"streaming_test\"\n",
    "streaming_collector = Online_Collector(streaming_table_name,ws=ws)\n",
    "\n",
    "import random\n",
    "streaming_collector.start_logging_df(buffer_time=2, batch_size=10)\n",
    "\n",
    "for run_id in [\"r000001\", \"r000002\", \"r000003\", \"r000004\", \"r000005\"]:\n",
    "    for i in range(1000):\n",
    "        for lr in [\"0.001\", \"0.002\"]:\n",
    "            df = pd.DataFrame({ \"timestamp\":pd.to_datetime('today'), \"lr\":[lr],\"metric1\":[random.uniform(3,50)] })\n",
    "            streaming_collector.stream_collect_df_queue(df)\n",
    "# streaming_collector.stop_logging()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.query import RT_Visualization\n",
    "rt_viz =RT_Visualization(streaming_table_name,ws)\n",
    "rt_viz.scatter(max_records=200, ago='12h',groupby='lr', y_metric='metric1',x_metric='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.query import KustoQuery\n",
    "query = KustoQuery(streaming_table_name, ws)\n",
    "\n",
    "query.anomaly_detection(min_t=\"4/26/2022 10:08:45\", max_t=\"4/23/2022 10:10:00\", step=\"0.5s\", metric=\"metric1\", agg=\"avg\", ts_col=\"timestamp\", groupby=\"lr\", sensitivity=0.1, filter=\"0.001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.drift_analysis import Drift_Analysis\n",
    "drift_analysis =Drift_Analysis(ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = drift_analysis.analyze_drift(limit=30000000,base_table_name = 'ISDWeather',tgt_table_name='ISDWeather', base_dt_from='2013-04-13', base_dt_to='2014-05-13', tgt_dt_from='2013-04-13', tgt_dt_to='2014-05-13', bin='30d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>numeric_feature</th>\n",
       "      <th>wasserstein</th>\n",
       "      <th>base_min</th>\n",
       "      <th>base_max</th>\n",
       "      <th>base_mean</th>\n",
       "      <th>target_min</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_mean</th>\n",
       "      <th>categorical_feature</th>\n",
       "      <th>euclidean</th>\n",
       "      <th>base_dcount</th>\n",
       "      <th>target_dcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-04-08 00:00:00+00:00</td>\n",
       "      <td>latitude</td>\n",
       "      <td>1.785103</td>\n",
       "      <td>-90</td>\n",
       "      <td>87</td>\n",
       "      <td>40</td>\n",
       "      <td>-90</td>\n",
       "      <td>87</td>\n",
       "      <td>40</td>\n",
       "      <td>usaf</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12320</td>\n",
       "      <td>11572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-04-08 00:00:00+00:00</td>\n",
       "      <td>latitude</td>\n",
       "      <td>1.785103</td>\n",
       "      <td>-90</td>\n",
       "      <td>87</td>\n",
       "      <td>40</td>\n",
       "      <td>-90</td>\n",
       "      <td>87</td>\n",
       "      <td>40</td>\n",
       "      <td>wban</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2404</td>\n",
       "      <td>2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-04-08 00:00:00+00:00</td>\n",
       "      <td>longitude</td>\n",
       "      <td>12.042559</td>\n",
       "      <td>-179</td>\n",
       "      <td>999</td>\n",
       "      <td>-80</td>\n",
       "      <td>-179</td>\n",
       "      <td>999</td>\n",
       "      <td>-80</td>\n",
       "      <td>usaf</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12320</td>\n",
       "      <td>11572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-04-08 00:00:00+00:00</td>\n",
       "      <td>longitude</td>\n",
       "      <td>12.042559</td>\n",
       "      <td>-179</td>\n",
       "      <td>999</td>\n",
       "      <td>-80</td>\n",
       "      <td>-179</td>\n",
       "      <td>999</td>\n",
       "      <td>-80</td>\n",
       "      <td>wban</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2404</td>\n",
       "      <td>2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-04-08 00:00:00+00:00</td>\n",
       "      <td>windSpeed</td>\n",
       "      <td>0.027431</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>usaf</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12320</td>\n",
       "      <td>11572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>2013-06-12 00:00:00+00:00</td>\n",
       "      <td>year_1</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>2013</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>wban</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2404</td>\n",
       "      <td>2366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>2013-06-12 00:00:00+00:00</td>\n",
       "      <td>day_1</td>\n",
       "      <td>5.572159</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>usaf</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12320</td>\n",
       "      <td>11534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2013-06-12 00:00:00+00:00</td>\n",
       "      <td>day_1</td>\n",
       "      <td>5.572159</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>wban</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2404</td>\n",
       "      <td>2366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>2013-06-12 00:00:00+00:00</td>\n",
       "      <td>version_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>usaf</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12320</td>\n",
       "      <td>11534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>2013-06-12 00:00:00+00:00</td>\n",
       "      <td>version_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wban</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2404</td>\n",
       "      <td>2366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    frequency numeric_feature  wasserstein  base_min  \\\n",
       "0   2014-04-08 00:00:00+00:00        latitude     1.785103       -90   \n",
       "1   2014-04-08 00:00:00+00:00        latitude     1.785103       -90   \n",
       "2   2014-04-08 00:00:00+00:00       longitude    12.042559      -179   \n",
       "3   2014-04-08 00:00:00+00:00       longitude    12.042559      -179   \n",
       "4   2014-04-08 00:00:00+00:00       windSpeed     0.027431         0   \n",
       "..                        ...             ...          ...       ...   \n",
       "415 2013-06-12 00:00:00+00:00          year_1     0.867188      2013   \n",
       "416 2013-06-12 00:00:00+00:00           day_1     5.572159         1   \n",
       "417 2013-06-12 00:00:00+00:00           day_1     5.572159         1   \n",
       "418 2013-06-12 00:00:00+00:00       version_1          0.0         1   \n",
       "419 2013-06-12 00:00:00+00:00       version_1          0.0         1   \n",
       "\n",
       "     base_max  base_mean  target_min  target_max  target_mean  \\\n",
       "0          87         40         -90          87           40   \n",
       "1          87         40         -90          87           40   \n",
       "2         999        -80        -179         999          -80   \n",
       "3         999        -80        -179         999          -80   \n",
       "4          90          3           0          79            3   \n",
       "..        ...        ...         ...         ...          ...   \n",
       "415      2014       2014        2013        2013         2013   \n",
       "416        31         14          12          30           21   \n",
       "417        31         14          12          30           21   \n",
       "418         1          1           1           1            1   \n",
       "419         1          1           1           1            1   \n",
       "\n",
       "    categorical_feature  euclidean  base_dcount  target_dcount  \n",
       "0                  usaf       28.0        12320          11572  \n",
       "1                  wban       28.0         2404           2364  \n",
       "2                  usaf       28.0        12320          11572  \n",
       "3                  wban       28.0         2404           2364  \n",
       "4                  usaf       28.0        12320          11572  \n",
       "..                  ...        ...          ...            ...  \n",
       "415                wban       28.0         2404           2366  \n",
       "416                usaf       28.0        12320          11534  \n",
       "417                wban       28.0         2404           2366  \n",
       "418                usaf       28.0        12320          11534  \n",
       "419                wban       28.0         2404           2366  \n",
       "\n",
       "[420 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html, Input, Output, State\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "from monitoring.drift_analysis import Drift_Analysis\n",
    "drift_analysis =Drift_Analysis(ws)\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "tables_list = drift_analysis.list_tables()\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                tables_list,\n",
    "                tables_list[0],\n",
    "                id='tables',\n",
    "            ),\n",
    "            dcc.DatePickerRange(\n",
    "            id='timeline',\n",
    "\n",
    "            ),\n",
    "            html.Button('Prepare data', id='prepare_data', n_clicks=0),\n",
    "            dcc.Store(id='intermediate-value')\n",
    "        ], style={'width': '20%', 'display': 'inline-block'}),\n",
    "\n",
    "        html.Div(\n",
    "\n",
    "            [ dcc.Dropdown(\n",
    "                id='columns'\n",
    "            ),\n",
    "            dcc.Dropdown(id='metrics'),\n",
    "            dcc.Store(id='column-name-type')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ], style={'width': '20%', 'float': 'right', 'display': 'inline-block'})\n",
    "    ]),\n",
    "\n",
    "    dcc.Graph(id='graph'),\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('columns', 'options'),\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    return drift_analysis.list_table_columns(table_name)['AttributeName'].values\n",
    "@app.callback(\n",
    "    Output('column-name-type', 'data'),\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    return drift_analysis.list_table_columns(table_name).to_json(date_format='iso', orient='split')\n",
    "\n",
    "@app.callback(\n",
    "    Output('columns', 'value'),\n",
    "    Input('columns', 'options'))\n",
    "def set_columns_value(available_options):\n",
    "    return available_options[0]\n",
    "\n",
    "@app.callback(\n",
    "    Output('metrics', 'options'),\n",
    "    Input('columns', 'value'),\n",
    "    Input('column-name-type', 'data'),\n",
    ")\n",
    "def set_metric(column, column_dict):\n",
    "    column_df = pd.read_json(column_dict, orient='split')\n",
    "    column_type = str(column_df[column_df['AttributeName']==column]['AttributeType'][0])\n",
    "    print(\"column type \", column_type)\n",
    "    if column_type=='StringBuffer':\n",
    "        return ['Distinct_count', 'Euclidean']\n",
    "    elif column_type=='DateTime':\n",
    "        return ['NA']\n",
    "    else:\n",
    "        return ['Max', 'Min', \"Mean\", \"wasserstein\"]\n",
    "\n",
    "@app.callback(\n",
    "    [Output('timeline', 'min_date_allowed'),Output('timeline', 'max_date_allowed'),Output('timeline', 'initial_visible_month'),Output('timeline', 'start_date'), Output('timeline', 'end_date')],\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    time_range =drift_analysis.get_time_range(table_name)\n",
    "\n",
    "    start_date = parser.parse(time_range[0]).replace(microsecond=0, second=0, minute=0)\n",
    "    end_date = parser.parse(time_range[1]).replace(microsecond=0, second=0, minute=0)\n",
    "    \n",
    "    return [start_date, end_date,end_date, start_date, end_date]\n",
    "@app.callback(\n",
    "    Output('intermediate-value', 'data'),\n",
    "    State('tables', 'value'),\n",
    "    State('timeline', 'start_date'),\n",
    "    State('timeline', 'end_date'), \n",
    "    Input('prepare_data', 'n_clicks')\n",
    "    \n",
    "    )\n",
    "def calculate(table_name, start_date, end_date,n_clicks):\n",
    "\n",
    "    query_df = drift_analysis.analyze_drift(limit=100000,base_table_name = f'{table_name}',tgt_table_name=f'{table_name}', base_dt_from=f'{start_date}', base_dt_to=f'{end_date}', tgt_dt_from=f'{start_date}', tgt_dt_to=f'{end_date}', bin='30d')\n",
    "    return query_df.to_json(date_format='iso', orient='split')\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph', 'figure'),\n",
    "    Input('columns', 'value'),\n",
    "    Input('intermediate-value', 'data'),\n",
    "    prevent_initial_call=True\n",
    "\n",
    "\n",
    "    )\n",
    "def update_figure(column,jsonified_cleaned_data):\n",
    "    dff = pd.read_json(jsonified_cleaned_data, orient='split')\n",
    "    dff.sort_values(\"frequency\", inplace=True)\n",
    "    if dff[dff['numeric_feature']==column].shape[0]>0:\n",
    "        y_values = dff[dff['numeric_feature']==column]['wasserstein']\n",
    "        x_values= dff[dff['numeric_feature']==column][\"frequency\"]\n",
    "    elif dff[dff['categorical_feature']==column].shape[0]>0:\n",
    "        y_values = dff[dff['categorical_feature']==column]['target_dcount']\n",
    "\n",
    "        x_values = dff[dff['categorical_feature']==column][\"frequency\"]\n",
    "    else:\n",
    "        x_values =[0]\n",
    "        y_values =[0]\n",
    "\n",
    "    fig = go.Figure([go.Scatter(x=x_values, y=y_values, name=f\"{column}\")])\n",
    "\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "app.run_server()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7f364c9551711cd4699acda32e0312c3edab483ae246bf330de758088cecccb"
  },
  "kernelspec": {
   "display_name": "dlresearch",
   "language": "python",
   "name": "dlresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
