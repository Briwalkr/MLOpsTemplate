{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nserafino\\Anaconda3\\envs\\mlopsenv\\lib\\site-packages\\cryptography\\hazmat\\_der.py:9: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes, int_to_bytes\n",
      "C:\\Users\\nserafino\\Anaconda3\\envs\\mlopsenv\\lib\\site-packages\\pkg_resources\\__init__.py:122: PkgResourcesDeprecationWarning: win32 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception The 'applicationinsights' distribution was not found and is required by azureml-telemetry.\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (urllib3 1.26.8 (c:\\users\\nserafino\\anaconda3\\envs\\mlopsenv\\lib\\site-packages), Requirement.parse('urllib3<=1.26.7,>=1.23')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (urllib3 1.26.8 (c:\\users\\nserafino\\anaconda3\\envs\\mlopsenv\\lib\\site-packages), Requirement.parse('urllib3<=1.26.7,>=1.23'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (urllib3 1.26.8 (c:\\users\\nserafino\\anaconda3\\envs\\mlopsenv\\lib\\site-packages), Requirement.parse('urllib3<=1.26.7,>=1.23'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (urllib3 1.26.8 (c:\\users\\nserafino\\anaconda3\\envs\\mlopsenv\\lib\\site-packages), Requirement.parse('urllib3<=1.26.7,>=1.23'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception The 'applicationinsights' distribution was not found and is required by azureml-telemetry.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "kv=ws.get_default_keyvault()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisite\n",
    "If you use the provision feature, then just need a workspace ws object\n",
    "If you need to create the cluster manually\n",
    "    1. Create a service principal and secret (SP)\n",
    "    2. Provision an ADX cluster and create a DB\n",
    "    3. Add the SP to be contributor of the cluster\n",
    "    4. pip install following packages: pip install --upgrade azure-mgmt-kusto azure-kusto-ingest azure-kusto-data azure-identity \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provisioning resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provision ADX cluster and register neccessary information at the workspace's keyvault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Option to set parameters to a custom ADX cluster\n",
    "# from monitoring import KV_SP_ID, KV_SP_KEY, KV_ADX_DB, KV_ADX_URI, KV_TENANT_ID\n",
    "# tenant_id = \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
    "# #Application ID\n",
    "# client_id = \"af883abf-89dd-4889-bdb3-1ee84f68465e\"\n",
    "# #Client Secret, set it at your WS' keyvault with key name same as your client_id\n",
    "# client_secret = kv.get_secret(client_id)\n",
    "# subscription_id = \"0e9bace8-7a81-4922-83b5-d995ff706507\"\n",
    "\n",
    "# cluster_uri = \"https://adx02.westus2.kusto.windows.net\" #URL of the ADX Cluster\n",
    "# kv.set_secret(KV_SP_ID,client_id)\n",
    "# kv.set_secret(KV_SP_KEY,client_secret)\n",
    "# kv.set_secret(KV_ADX_DB,\"db01\")\n",
    "# kv.set_secret(KV_ADX_URI,cluster_uri)\n",
    "# kv.set_secret(KV_TENANT_ID,tenant_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.management import provision\n",
    "provision(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.data_collector import Online_Collector\n",
    "table_name = \"isd_weather_test4\" #new dataset\n",
    "\n",
    "sample_pd_data = pd.read_parquet(\"data/test_data.parquet\").head(10)\n",
    "sample_pd_data['timestamp'] = sample_pd_data['datetime']\n",
    "sample_pd_data.drop(['datetime'], inplace=True, axis=1)\n",
    "\n",
    "online_collector = Online_Collector(table_name,ws=ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://adx02.westus2.kusto.windows.net'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_collector.cluster_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Online_Collector' object has no attribute 'stream_collect_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nserafino\\Documents\\Other\\github-mlops\\GitHub\\src\\utilities\\test\\test.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nserafino/Documents/Other/github-mlops/GitHub/src/utilities/test/test.ipynb#ch0000010?line=0'>1</a>\u001b[0m online_collector\u001b[39m.\u001b[39;49mstream_collect_df(sample_pd_data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Online_Collector' object has no attribute 'stream_collect_df'"
     ]
    }
   ],
   "source": [
    "online_collector.stream_collect_df(sample_pd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.batch_collect(sample_pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Ingestion and Real Time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'ws'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nserafino\\Documents\\Other\\github-mlops\\GitHub\\src\\utilities\\test\\test.ipynb Cell 15'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nserafino/Documents/Other/github-mlops/GitHub/src/utilities/test/test.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39m# Ingest streaming data  asynchronously with internal buffering mechanism to lower impact to main scoring thread\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nserafino/Documents/Other/github-mlops/GitHub/src/utilities/test/test.ipynb#ch0000013?line=1'>2</a>\u001b[0m streaming_table_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstreaming_test\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nserafino/Documents/Other/github-mlops/GitHub/src/utilities/test/test.ipynb#ch0000013?line=2'>3</a>\u001b[0m streaming_collector \u001b[39m=\u001b[39m Online_Collector(streaming_table_name,ws\u001b[39m=\u001b[39;49mws)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nserafino/Documents/Other/github-mlops/GitHub/src/utilities/test/test.ipynb#ch0000013?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nserafino/Documents/Other/github-mlops/GitHub/src/utilities/test/test.ipynb#ch0000013?line=5'>6</a>\u001b[0m streaming_collector\u001b[39m.\u001b[39mstart_logging_df(buffer_time\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ws'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ingest streaming data  asynchronously with internal buffering mechanism to lower impact to main scoring thread\n",
    "streaming_table_name=\"streaming_test\"\n",
    "streaming_collector = Online_Collector(streaming_table_name,ws=ws)\n",
    "\n",
    "import random\n",
    "streaming_collector.start_logging_df(buffer_time=2, batch_size=10)\n",
    "\n",
    "for run_id in [\"r000001\", \"r000002\", \"r000003\", \"r000004\", \"r000005\"]:\n",
    "    for i in range(1000):\n",
    "        for lr in [\"0.001\", \"0.002\"]:\n",
    "            df = pd.DataFrame({ \"timestamp\":pd.to_datetime('today'), \"lr\":[lr],\"metric1\":[random.uniform(3,50)] })\n",
    "            streaming_collector.stream_collect_df_queue(df)\n",
    "# streaming_collector.stop_logging()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nserafino\\AppData\\Local\\Temp\\ipykernel_115072\\1097353187.py:1: FutureWarning:\n",
      "\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>avg_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 days 00:00:00</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>-2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 days 00:00:00</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 days 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>-9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10 days</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               day avg_temperature\n",
       "0  2 days 00:00:00             0.1\n",
       "1  0 days 00:00:00             0.5\n",
       "2  1 days 00:00:00           -2.25\n",
       "3  5 days 00:00:00             1.3\n",
       "4  6 days 00:00:00             1.0\n",
       "5  4 days 00:00:00            -9.5\n",
       "6          10 days       [1, 2, 3]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest = df.append(pd.Series(data=[\"10 days\", [1,2,3]], index=[\"day\", \"avg_temperature\"]), ignore_index=True)\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.avg_temperature.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categorical_feature</th>\n",
       "      <th>base_categorical_feature_value</th>\n",
       "      <th>base_dcount</th>\n",
       "      <th>period2</th>\n",
       "      <th>categorical_feature1</th>\n",
       "      <th>target_categorical_feature_value</th>\n",
       "      <th>target_dcount</th>\n",
       "      <th>basearraylength</th>\n",
       "      <th>targetarraylength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usaf</td>\n",
       "      <td>[711740,122260,749484,724673,722140,603900,584...</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-04-13T00:00:00Z</td>\n",
       "      <td>usaf</td>\n",
       "      <td>[711740,722140,711740,225500,584570,747946,724...</td>\n",
       "      <td>15</td>\n",
       "      <td>1000</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usaf</td>\n",
       "      <td>[711740,122260,749484,724673,722140,603900,584...</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-05-13T00:00:00Z</td>\n",
       "      <td>usaf</td>\n",
       "      <td>[122260,749484,724673,603900,584570,722010,749...</td>\n",
       "      <td>15</td>\n",
       "      <td>1000</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wban</td>\n",
       "      <td>[99999,99999,395,93009,93805,99999,99999,99999...</td>\n",
       "      <td>8</td>\n",
       "      <td>2013-04-13T00:00:00Z</td>\n",
       "      <td>wban</td>\n",
       "      <td>[99999,93805,99999,99999,99999,12886,13733,395...</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wban</td>\n",
       "      <td>[99999,99999,395,93009,93805,99999,99999,99999...</td>\n",
       "      <td>8</td>\n",
       "      <td>2013-05-13T00:00:00Z</td>\n",
       "      <td>wban</td>\n",
       "      <td>[99999,395,93009,99999,99999,12836,395,99999,9...</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  categorical_feature                     base_categorical_feature_value  \\\n",
       "0                usaf  [711740,122260,749484,724673,722140,603900,584...   \n",
       "1                usaf  [711740,122260,749484,724673,722140,603900,584...   \n",
       "2                wban  [99999,99999,395,93009,93805,99999,99999,99999...   \n",
       "3                wban  [99999,99999,395,93009,93805,99999,99999,99999...   \n",
       "\n",
       "   base_dcount               period2 categorical_feature1  \\\n",
       "0           15  2013-04-13T00:00:00Z                 usaf   \n",
       "1           15  2013-05-13T00:00:00Z                 usaf   \n",
       "2            8  2013-04-13T00:00:00Z                 wban   \n",
       "3            8  2013-05-13T00:00:00Z                 wban   \n",
       "\n",
       "                    target_categorical_feature_value  target_dcount  \\\n",
       "0  [711740,722140,711740,225500,584570,747946,724...             15   \n",
       "1  [122260,749484,724673,603900,584570,722010,749...             15   \n",
       "2  [99999,93805,99999,99999,99999,12886,13733,395...              8   \n",
       "3  [99999,395,93009,99999,99999,12836,395,99999,9...              8   \n",
       "\n",
       "   basearraylength  targetarraylength  \n",
       "0             1000                381  \n",
       "1             1000                619  \n",
       "2             1000                381  \n",
       "3             1000                619  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfread = pd.read_csv(\"C:/Users/nserafino/Downloads/export - 2022-05-05T140742.247.csv\")\n",
    "dfread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = dfread.shape[0]\n",
    "distance1 = []\n",
    "for i in range(n):\n",
    "    base_features = [int(x.strip(\"[\").strip(\"]\")) for x in dfread[\"base_categorical_feature_value\"][i].split(\",\")]\n",
    "    target_features = [int(x.strip(\"[\").strip(\"]\")) for x in dfread[\"target_categorical_feature_value\"][i].split(\",\")]\n",
    "    if len(target_features) > len(base_features):\n",
    "        target_features = random.sample(target_features, len(base_features))\n",
    "    elif len(target_features) < len(base_features):\n",
    "        base_features = random.sample(base_features, len(target_features))\n",
    "\n",
    "    distance1.append(distance.euclidean(target_features, base_features))\n",
    "    #     print(type(int(j.strip(\"[\").strip(\"]\"))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6295999.0974449795, 8439608.529132202, 1165274.6175211233, 1452404.0810938256]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x25ec82dfbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from monitoring.query import RT_Visualization\n",
    "rt_viz =RT_Visualization(streaming_table_name,ws)\n",
    "rt_viz.scatter(max_records=200, ago='12h',groupby='lr', y_metric='metric1',x_metric='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.query import KustoQuery\n",
    "query = KustoQuery(streaming_table_name, ws)\n",
    "\n",
    "query.anomaly_detection(min_t=\"4/26/2022 10:08:45\", max_t=\"4/23/2022 10:10:00\", step=\"0.5s\", metric=\"metric1\", agg=\"avg\", ts_col=\"timestamp\", groupby=\"lr\", sensitivity=0.1, filter=\"0.001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.drift_analysis import Drift_Analysis\n",
    "drift_analysis =Drift_Analysis(ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>9963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>12466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>9969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>11485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>9968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EventId\n",
       "0       13208\n",
       "1       23358\n",
       "2       23357\n",
       "3        9494\n",
       "4        9488\n",
       "...       ...\n",
       "3261     9963\n",
       "3262    12466\n",
       "3263     9969\n",
       "3264    11485\n",
       "3265     9968\n",
       "\n",
       "[3266 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drift_analysis.sample_data_points(\"StormEvents\", col_name=\"EventId\", start_date = \"01/01/2007\", horizon=30 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = drift_analysis.analyze_drift(limit=30000000,base_table_name = 'ISDWeather',tgt_table_name='ISDWeather', base_dt_from='2013-04-13', base_dt_to='2014-05-13', tgt_dt_from='2013-04-13', tgt_dt_to='2014-05-13', bin='30d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "import dash\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html, Input, Output, State\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "from monitoring.drift_analysis import Drift_Analysis\n",
    "drift_analysis =Drift_Analysis(ws)\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "tables_list = drift_analysis.list_tables()\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Tabs([\n",
    "        dcc.Tab(label='Dataset Drift', children=[\n",
    "               html.Div([\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                tables_list,\n",
    "                tables_list[0],\n",
    "                id='table_option',\n",
    "            )]),\n",
    "            \n",
    "            html.Div([ dcc.Dropdown(\n",
    "                id='columns_option',\n",
    "                multi=True\n",
    "            )],),\n",
    "\n",
    "        ]),\n",
    "        dcc.Graph(id='graph_overview'),]),\n",
    "        dcc.Tab(label='Feature Analysis', children=[\n",
    "    html.Div([\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                tables_list,\n",
    "                tables_list[0],\n",
    "                id='tables',\n",
    "            ),\n",
    "            dcc.DatePickerRange(\n",
    "            id='base_timeline',start_date_placeholder_text =\"base_start_date\",end_date_placeholder_text =\"base_end_date\"\n",
    "            \n",
    "\n",
    "            ),\n",
    "            dcc.DatePickerRange(\n",
    "            id='target_timeline',start_date_placeholder_text =\"target_start_date\",end_date_placeholder_text =\"target_end_date\"\n",
    "\n",
    "            ),\n",
    "            html.Button('Prepare data', id='prepare_data', n_clicks=0),\n",
    "            dcc.Store(id='intermediate-value')\n",
    "        ], style={'width': '20%', 'display': 'inline-block'}),\n",
    "\n",
    "        html.Div(\n",
    "\n",
    "            [ dcc.Dropdown(\n",
    "                id='columns'\n",
    "            ),\n",
    "            dcc.Dropdown(id='metrics'),\n",
    "            dcc.Store(id='column-name-type')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ], style={'width': '20%', 'float': 'right', 'display': 'inline-block'}),\n",
    "        dcc.Graph(id='graph')\n",
    "\n",
    "    ]),\n",
    "    html.Div([\n",
    "\n",
    "    dcc.Graph(id='detail_graph')\n",
    "    ])\n",
    "\n",
    "])\n",
    "    ])])\n",
    "\n",
    "# overview tab\n",
    "@app.callback(\n",
    "    Output('columns_option', 'options'),\n",
    "    Input('table_option', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    return drift_analysis.list_table_columns(table_name)['AttributeName'].values\n",
    "\n",
    "@app.callback(\n",
    "    Output('columns_option', 'value'),\n",
    "    Input('columns_option', 'options'))\n",
    "def set_columns_value(available_options):\n",
    "    return available_options\n",
    "\n",
    "# feature analysis tab\n",
    "@app.callback(\n",
    "    Output('columns', 'options'),\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    return drift_analysis.list_table_columns(table_name)['AttributeName'].values\n",
    "@app.callback(\n",
    "    Output('column-name-type', 'data'),\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    return drift_analysis.list_table_columns(table_name).to_json(date_format='iso', orient='split')\n",
    "\n",
    "@app.callback(\n",
    "    Output('columns', 'value'),\n",
    "    Input('columns', 'options'))\n",
    "def set_columns_value(available_options):\n",
    "    return available_options[0]\n",
    "\n",
    "@app.callback(\n",
    "    Output('metrics', 'options'),\n",
    "    Input('columns', 'value'),\n",
    "    Input('column-name-type', 'data'),\n",
    ")\n",
    "def set_metric_options(column, column_dict):\n",
    "    column_df = pd.read_json(column_dict, orient='split')\n",
    "    column_type = str(column_df[column_df['AttributeName']==column]['AttributeType'].values[0])\n",
    "    if column_type=='StringBuffer':\n",
    "        return ['dcount', 'euclidean']\n",
    "    elif column_type=='DateTime':\n",
    "        return ['NA']\n",
    "    else:\n",
    "        return ['max', 'min', \"mean\", \"wasserstein\"]\n",
    "@app.callback(\n",
    "    Output('metrics', 'value'),\n",
    "    Input('metrics', 'options'))\n",
    "def set_columns_value(available_options):\n",
    "    return available_options[0]\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [Output('base_timeline', 'min_date_allowed'),Output('base_timeline', 'max_date_allowed'),Output('base_timeline', 'initial_visible_month'),Output('base_timeline', 'start_date'), Output('base_timeline', 'end_date'),\n",
    "    Output('target_timeline', 'min_date_allowed'),Output('target_timeline', 'max_date_allowed'),Output('target_timeline', 'initial_visible_month'),Output('target_timeline', 'start_date'), Output('target_timeline', 'end_date')],\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    time_range =drift_analysis.get_time_range(table_name)\n",
    "\n",
    "    start_date = parser.parse(time_range[0]).replace(microsecond=0, second=0, minute=0)\n",
    "    end_date = parser.parse(time_range[1]).replace(microsecond=0, second=0, minute=0)\n",
    "    \n",
    "    return [start_date, end_date,end_date, start_date, end_date,start_date, end_date,end_date, start_date, end_date]\n",
    "@app.callback(\n",
    "    Output('intermediate-value', 'data'),\n",
    "    State('tables', 'value'),\n",
    "    State('base_timeline', 'start_date'),\n",
    "    State('base_timeline', 'end_date'), \n",
    "    State('target_timeline', 'start_date'),\n",
    "    State('target_timeline', 'end_date'), \n",
    "    Input('prepare_data', 'n_clicks'),\n",
    "        prevent_initial_call=True\n",
    "\n",
    "    \n",
    "    )\n",
    "def calculate(table_name, base_start_date, base_end_date,target_start_date, target_end_date,n_clicks):\n",
    "            # self,base_table_name,target_table_name, base_dt_from, base_dt_to, target_dt_from, target_dt_to, bin, limit=10000000, concurrent_run=True)\n",
    "    query_df = drift_analysis.analyze_drift(limit=100000,base_table_name = f'{table_name}', target_table_name=f'{table_name}', base_dt_from=f'{base_start_date}', base_dt_to=f'{base_end_date}', target_dt_from=f'{target_start_date}', target_dt_to=f'{target_end_date}', bin='30d')\n",
    "    return query_df.to_json(date_format='iso', orient='split')\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph', 'figure'),\n",
    "    Input('columns', 'value'),\n",
    "    Input('metrics', 'value'),\n",
    "    Input('intermediate-value', 'data'),\n",
    "    prevent_initial_call=True\n",
    "\n",
    "\n",
    "    )\n",
    "def update_figure(column,metric, jsonified_cleaned_data):\n",
    "    x_values =[0]\n",
    "    y_values =[0]\n",
    "    if jsonified_cleaned_data is not None:\n",
    "        dff = pd.read_json(jsonified_cleaned_data, orient='split')\n",
    "        dff.sort_values(\"target_start_date\", inplace=True)\n",
    "        if metric != \"wasserstein\" and metric != \"euclidean\" and metric !='NA':\n",
    "            metric = \"target_\"+metric\n",
    "        if dff[dff['numeric_feature']==column].shape[0]>0:\n",
    "            y_values = dff[dff['numeric_feature']==column][metric]\n",
    "            x_values= dff[dff['numeric_feature']==column][\"target_start_date\"]\n",
    "        elif dff[dff['categorical_feature']==column].shape[0]>0:\n",
    "            y_values = dff[dff['categorical_feature']==column][metric]\n",
    "\n",
    "            x_values = dff[dff['categorical_feature']==column][\"target_start_date\"]\n",
    "\n",
    "\n",
    "    fig = go.Figure([go.Scatter(x=x_values, y=y_values, name=f\"{column}\")])\n",
    "\n",
    "\n",
    "    return fig\n",
    "@app.callback(\n",
    "    Output('detail_graph', 'figure'),\n",
    "    Input('graph', 'clickData'),   \n",
    "    Input('tables', 'value'),\n",
    "    Input('columns', 'value'),\n",
    "\n",
    "\n",
    ")\n",
    "def drilldown(click_data,table, column):\n",
    "\n",
    "    # using callback context to check which input was fired\n",
    "    ctx = dash.callback_context\n",
    "    trigger_id = ctx.triggered[0][\"prop_id\"].split(\".\")[0]\n",
    "\n",
    "    if trigger_id == 'graph':\n",
    "\n",
    "        if click_data is not None:\n",
    "            start_date = click_data['points'][0]['x']\n",
    "            output = drift_analysis.sample_data_points(table,column,start_date,horizon=30)\n",
    "            output.dropna(inplace=True)\n",
    "            fig = px.histogram(output,x=column )\n",
    "            return fig\n",
    "\n",
    "    else:\n",
    "        fig = go.Figure(data=[go.Histogram(x=[0] )])\n",
    "        return fig\n",
    "\n",
    "app.run_server()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7f364c9551711cd4699acda32e0312c3edab483ae246bf330de758088cecccb"
  },
  "kernelspec": {
   "display_name": "dlresearch",
   "language": "python",
   "name": "dlresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
