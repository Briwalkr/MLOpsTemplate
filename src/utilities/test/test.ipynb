{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 7.0.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('pyarrow<4.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "kv=ws.get_default_keyvault()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisite\n",
    "If you use the provision feature, then just need a workspace ws object\n",
    "If you need to create the cluster manually\n",
    "    1. Create a service principal and secret (SP)\n",
    "    2. Provision an ADX cluster and create a DB\n",
    "    3. Add the SP to be contributor of the cluster\n",
    "    4. pip install following packages: pip install --upgrade azure-mgmt-kusto azure-kusto-ingest azure-kusto-data azure-identity \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provisioning resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provision ADX cluster and register neccessary information at the workspace's keyvault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Option to set parameters to a custom ADX cluster\n",
    "# from monitoring import KV_SP_ID, KV_SP_KEY, KV_ADX_DB, KV_ADX_URI, KV_TENANT_ID\n",
    "# tenant_id = \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
    "# #Application ID\n",
    "# client_id = \"af883abf-89dd-4889-bdb3-1ee84f68465e\"\n",
    "# #Client Secret, set it at your WS' keyvault with key name same as your client_id\n",
    "# client_secret = kv.get_secret(client_id)\n",
    "# subscription_id = \"0e9bace8-7a81-4922-83b5-d995ff706507\"\n",
    "\n",
    "# cluster_uri = \"https://adx02.westus2.kusto.windows.net\" #URL of the ADX Cluster\n",
    "# kv.set_secret(KV_SP_ID,client_id)\n",
    "# kv.set_secret(KV_SP_KEY,client_secret)\n",
    "# kv.set_secret(KV_ADX_DB,\"db01\")\n",
    "# kv.set_secret(KV_ADX_URI,cluster_uri)\n",
    "# kv.set_secret(KV_TENANT_ID,tenant_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.management import provision\n",
    "provision(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.data_collector import Online_Collector\n",
    "table_name = \"isd_weather_test4\" #new dataset\n",
    "\n",
    "sample_pd_data = pd.read_parquet(\"data/test_data.parquet\").head(10)\n",
    "sample_pd_data['timestamp'] = sample_pd_data['datetime']\n",
    "sample_pd_data.drop(['datetime'], inplace=True, axis=1)\n",
    "\n",
    "online_collector = Online_Collector(table_name,ws=ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.cluster_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.stream_collect_df(sample_pd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.batch_collect(sample_pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Ingestion and Real Time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ingest streaming data  asynchronously with internal buffering mechanism to lower impact to main scoring thread\n",
    "streaming_table_name=\"streaming_test\"\n",
    "streaming_collector = Online_Collector(streaming_table_name,ws=ws)\n",
    "\n",
    "import random\n",
    "streaming_collector.start_logging_df(buffer_time=2, batch_size=10)\n",
    "\n",
    "for run_id in [\"r000001\", \"r000002\", \"r000003\", \"r000004\", \"r000005\"]:\n",
    "    for i in range(1000):\n",
    "        for lr in [\"0.001\", \"0.002\"]:\n",
    "            df = pd.DataFrame({ \"timestamp\":pd.to_datetime('today'), \"lr\":[lr],\"metric1\":[random.uniform(3,50)] })\n",
    "            streaming_collector.stream_collect_df_queue(df)\n",
    "# streaming_collector.stop_logging()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.query import RT_Visualization\n",
    "rt_viz =RT_Visualization(streaming_table_name,ws)\n",
    "rt_viz.scatter(max_records=200, ago='12h',groupby='lr', y_metric='metric1',x_metric='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.query import KustoQuery\n",
    "query = KustoQuery(streaming_table_name, ws)\n",
    "\n",
    "query.anomaly_detection(min_t=\"4/26/2022 10:08:45\", max_t=\"4/23/2022 10:10:00\", step=\"0.5s\", metric=\"metric1\", agg=\"avg\", ts_col=\"timestamp\", groupby=\"lr\", sensitivity=0.1, filter=\"0.001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.drift_analysis import Drift_Analysis\n",
    "drift_analysis =Drift_Analysis(ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_analysis.sample_data_points(\"StormEvents\", col_name=\"EventId\", start_date = \"01/01/2007\", horizon=30 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = drift_analysis.analyze_drift(limit=30000000,base_table_name = 'ISDWeather',tgt_table_name='ISDWeather', base_dt_from='2013-04-13', base_dt_to='2014-05-13', tgt_dt_from='2013-04-13', tgt_dt_to='2014-05-13', bin='30d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "import dash\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html, Input, Output, State\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "from monitoring.drift_analysis import Drift_Analysis\n",
    "drift_analysis =Drift_Analysis(ws)\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "tables_list = drift_analysis.list_tables()\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                tables_list,\n",
    "                tables_list[0],\n",
    "                id='tables',\n",
    "            ),\n",
    "            dcc.DatePickerRange(\n",
    "            id='base_timeline',start_date_placeholder_text =\"base_start_date\",end_date_placeholder_text =\"base_end_date\"\n",
    "            \n",
    "\n",
    "            ),\n",
    "            dcc.DatePickerRange(\n",
    "            id='target_timeline',start_date_placeholder_text =\"target_start_date\",end_date_placeholder_text =\"target_end_date\"\n",
    "\n",
    "            ),\n",
    "            html.Button('Prepare data', id='prepare_data', n_clicks=0),\n",
    "            dcc.Store(id='intermediate-value')\n",
    "        ], style={'width': '20%', 'display': 'inline-block'}),\n",
    "\n",
    "        html.Div(\n",
    "\n",
    "            [ dcc.Dropdown(\n",
    "                id='columns'\n",
    "            ),\n",
    "            dcc.Dropdown(id='metrics'),\n",
    "            dcc.Store(id='column-name-type')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ], style={'width': '20%', 'float': 'right', 'display': 'inline-block'})\n",
    "        ,    dcc.Graph(id='graph')\n",
    "\n",
    "    ]),\n",
    "    html.Div([\n",
    "\n",
    "    dcc.Graph(id='detail_graph')\n",
    "    ])\n",
    "\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('columns', 'options'),\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    return drift_analysis.list_table_columns(table_name)['AttributeName'].values\n",
    "@app.callback(\n",
    "    Output('column-name-type', 'data'),\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    return drift_analysis.list_table_columns(table_name).to_json(date_format='iso', orient='split')\n",
    "\n",
    "@app.callback(\n",
    "    Output('columns', 'value'),\n",
    "    Input('columns', 'options'))\n",
    "def set_columns_value(available_options):\n",
    "    return available_options[0]\n",
    "\n",
    "@app.callback(\n",
    "    Output('metrics', 'options'),\n",
    "    Input('columns', 'value'),\n",
    "    Input('column-name-type', 'data'),\n",
    ")\n",
    "def set_metric_options(column, column_dict):\n",
    "    column_df = pd.read_json(column_dict, orient='split')\n",
    "    column_type = str(column_df[column_df['AttributeName']==column]['AttributeType'].values[0])\n",
    "    if column_type=='StringBuffer':\n",
    "        return ['dcount', 'euclidean']\n",
    "    elif column_type=='DateTime':\n",
    "        return ['NA']\n",
    "    else:\n",
    "        return ['max', 'min', \"mean\", \"wasserstein\"]\n",
    "@app.callback(\n",
    "    Output('metrics', 'value'),\n",
    "    Input('metrics', 'options'))\n",
    "def set_columns_value(available_options):\n",
    "    return available_options[0]\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [Output('base_timeline', 'min_date_allowed'),Output('base_timeline', 'max_date_allowed'),Output('base_timeline', 'initial_visible_month'),Output('base_timeline', 'start_date'), Output('base_timeline', 'end_date'),\n",
    "    Output('target_timeline', 'min_date_allowed'),Output('target_timeline', 'max_date_allowed'),Output('target_timeline', 'initial_visible_month'),Output('target_timeline', 'start_date'), Output('target_timeline', 'end_date')],\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    time_range =drift_analysis.get_time_range(table_name)\n",
    "\n",
    "    start_date = parser.parse(time_range[0]).replace(microsecond=0, second=0, minute=0)\n",
    "    end_date = parser.parse(time_range[1]).replace(microsecond=0, second=0, minute=0)\n",
    "    \n",
    "    return [start_date, end_date,end_date, start_date, end_date,start_date, end_date,end_date, start_date, end_date]\n",
    "@app.callback(\n",
    "    Output('intermediate-value', 'data'),\n",
    "    State('tables', 'value'),\n",
    "    State('base_timeline', 'start_date'),\n",
    "    State('base_timeline', 'end_date'), \n",
    "    State('target_timeline', 'start_date'),\n",
    "    State('target_timeline', 'end_date'), \n",
    "    Input('prepare_data', 'n_clicks'),\n",
    "        prevent_initial_call=True\n",
    "\n",
    "    \n",
    "    )\n",
    "def calculate(table_name, base_start_date, base_end_date,target_start_date, target_end_date,n_clicks):\n",
    "\n",
    "    query_df = drift_analysis.analyze_drift(limit=100000,base_table_name = f'{table_name}',tgt_table_name=f'{table_name}', base_dt_from=f'{base_start_date}', base_dt_to=f'{base_end_date}', tgt_dt_from=f'{target_start_date}', tgt_dt_to=f'{target_end_date}', bin='30d')\n",
    "    return query_df.to_json(date_format='iso', orient='split')\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph', 'figure'),\n",
    "    Input('columns', 'value'),\n",
    "    Input('metrics', 'value'),\n",
    "    Input('intermediate-value', 'data'),\n",
    "    prevent_initial_call=True\n",
    "\n",
    "\n",
    "    )\n",
    "def update_figure(column,metric, jsonified_cleaned_data):\n",
    "    x_values =[0]\n",
    "    y_values =[0]\n",
    "    if jsonified_cleaned_data is not None:\n",
    "        dff = pd.read_json(jsonified_cleaned_data, orient='split')\n",
    "        dff.sort_values(\"frequency\", inplace=True)\n",
    "        if metric != \"wasserstein\" and metric != \"euclidean\" and metric !='NA':\n",
    "            metric = \"target_\"+metric\n",
    "        if dff[dff['numeric_feature']==column].shape[0]>0:\n",
    "            y_values = dff[dff['numeric_feature']==column][metric]\n",
    "            x_values= dff[dff['numeric_feature']==column][\"frequency\"]\n",
    "        elif dff[dff['categorical_feature']==column].shape[0]>0:\n",
    "            y_values = dff[dff['categorical_feature']==column][metric]\n",
    "\n",
    "            x_values = dff[dff['categorical_feature']==column][\"frequency\"]\n",
    "\n",
    "\n",
    "    fig = go.Figure([go.Scatter(x=x_values, y=y_values, name=f\"{column}\")])\n",
    "\n",
    "\n",
    "    return fig\n",
    "@app.callback(\n",
    "    Output('detail_graph', 'figure'),\n",
    "    Input('graph', 'clickData'),   \n",
    "    Input('tables', 'value'),\n",
    "    Input('columns', 'value'),\n",
    "\n",
    "\n",
    ")\n",
    "def drilldown(click_data,table, column):\n",
    "\n",
    "    # using callback context to check which input was fired\n",
    "    ctx = dash.callback_context\n",
    "    trigger_id = ctx.triggered[0][\"prop_id\"].split(\".\")[0]\n",
    "\n",
    "    if trigger_id == 'graph':\n",
    "\n",
    "        if click_data is not None:\n",
    "            start_date = click_data['points'][0]['x']\n",
    "            output = drift_analysis.sample_data_points(table,column,start_date,horizon=30)\n",
    "            output.dropna(inplace=True)\n",
    "            fig = px.histogram(output,x=column )\n",
    "            return fig\n",
    "\n",
    "    else:\n",
    "        fig = go.Figure(data=[go.Histogram(x=[0] )])\n",
    "        return fig\n",
    "\n",
    "app.run_server()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7f364c9551711cd4699acda32e0312c3edab483ae246bf330de758088cecccb"
  },
  "kernelspec": {
   "display_name": "dlresearch",
   "language": "python",
   "name": "dlresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
