{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 7.0.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('pyarrow<4.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "kv=ws.get_default_keyvault()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisite\n",
    "If you use the provision feature, then just need a workspace ws object\n",
    "If you need to create the cluster manually\n",
    "    1. Create a service principal and secret (SP)\n",
    "    2. Provision an ADX cluster and create a DB\n",
    "    3. Add the SP to be contributor of the cluster\n",
    "    4. pip install following packages: pip install --upgrade azure-mgmt-kusto azure-kusto-ingest azure-kusto-data azure-identity \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provisioning resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provision ADX cluster and register neccessary information at the workspace's keyvault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option to set parameters to a custom ADX cluster\n",
    "# from monitoring import KV_SP_ID, KV_SP_KEY, KV_ADX_DB, KV_ADX_URI, KV_TENANT_ID\n",
    "# tenant_id = \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
    "# #Application ID\n",
    "# client_id = \"af883abf-89dd-4889-bdb3-1ee84f68465e\"\n",
    "# #Client Secret, set it at your WS' keyvault with key name same as your client_id\n",
    "# client_secret = kv.get_secret(client_id)\n",
    "# subscription_id = \"0e9bace8-7a81-4922-83b5-d995ff706507\"\n",
    "\n",
    "# cluster_uri = \"https://adx02.westus2.kusto.windows.net\" #URL of the ADX Cluster\n",
    "# kv.set_secret(KV_SP_ID,client_id)\n",
    "# kv.set_secret(KV_SP_KEY,client_secret)\n",
    "# kv.set_secret(KV_ADX_DB,\"db01\")\n",
    "# kv.set_secret(KV_ADX_URI,cluster_uri)\n",
    "# kv.set_secret(KV_TENANT_ID,tenant_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.management import provision\n",
    "provision(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.data_collector import Online_Collector\n",
    "table_name = \"isd_weather_test4\" #new dataset\n",
    "\n",
    "sample_pd_data = pd.read_parquet(\"data/test_data.parquet\").head(10)\n",
    "sample_pd_data['timestamp'] = sample_pd_data['datetime']\n",
    "sample_pd_data.drop(['datetime'], inplace=True, axis=1)\n",
    "online_collector = Online_Collector(table_name,ws=ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.stream_collect_df(sample_pd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_collector.batch_collect(sample_pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Ingestion and Real Time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ingest streaming data  asynchronously with internal buffering mechanism to lower impact to main scoring thread\n",
    "streaming_table_name=\"streaming_test\"\n",
    "streaming_collector = Online_Collector(streaming_table_name,ws=ws)\n",
    "\n",
    "import random\n",
    "streaming_collector.start_logging_df(buffer_time=2, batch_size=10)\n",
    "\n",
    "for run_id in [\"r000001\", \"r000002\", \"r000003\", \"r000004\", \"r000005\"]:\n",
    "    for i in range(1000):\n",
    "        for lr in [\"0.001\", \"0.002\"]:\n",
    "            df = pd.DataFrame({ \"timestamp\":pd.to_datetime('today'), \"lr\":[lr],\"metric1\":[random.uniform(3,50)] })\n",
    "            streaming_collector.stream_collect_df_queue(df)\n",
    "# streaming_collector.stop_logging()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.query import RT_Visualization\n",
    "rt_viz =RT_Visualization(streaming_table_name,ws)\n",
    "rt_viz.scatter(max_records=200, ago='12h',groupby='lr', y_metric='metric1',x_metric='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.query import KustoQuery\n",
    "query = KustoQuery(streaming_table_name, ws)\n",
    "\n",
    "query.anomaly_detection(min_t=\"4/26/2022 10:08:45\", max_t=\"4/23/2022 10:10:00\", step=\"0.5s\", metric=\"metric1\", agg=\"avg\", ts_col=\"timestamp\", groupby=\"lr\", sensitivity=0.1, filter=\"0.001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoring.drift_analysis import Drift_Analysis\n",
    "drift_analysis =Drift_Analysis(ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:55] \"\u001b[37mGET /_alive_2be179b8-5b5a-417e-9571-3f0ec0ef5ef0 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2da8408c9a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Apr/2022 14:24:55] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-datepicker.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:24:56] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:25:05] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:25:05] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2022 14:25:05] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "tables_list = drift_analysis.list_tables()\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                tables_list,\n",
    "                tables_list[0],\n",
    "                id='tables',\n",
    "            ),\n",
    "            dcc.Dropdown(\n",
    "                id='columns'\n",
    "            )\n",
    "        ], style={'width': '20%', 'display': 'inline-block'}),\n",
    "\n",
    "        html.Div([    dcc.DatePickerRange(\n",
    "        id='timeline',\n",
    "\n",
    "    ),\n",
    "\n",
    "        ], style={'width': '20%', 'float': 'right', 'display': 'inline-block'})\n",
    "    ]),\n",
    "\n",
    "    dcc.Graph(id='graph'),\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('columns', 'options'),\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    return drift_analysis.list_table_columns(table_name)['AttributeName'].values\n",
    "\n",
    "@app.callback(\n",
    "    Output('columns', 'value'),\n",
    "    Input('columns', 'options'))\n",
    "def set_columns_value(available_options):\n",
    "    return available_options[0]\n",
    "@app.callback(\n",
    "    [Output('timeline', 'min_date_allowed'),Output('timeline', 'max_date_allowed'),Output('timeline', 'initial_visible_month'),Output('timeline', 'end_date')],\n",
    "    Input('tables', 'value'))\n",
    "def set_columns_options(table_name):\n",
    "    time_range =drift_analysis.get_time_range(table_name)\n",
    "\n",
    "    start_date = parser.parse(time_range[0]).replace(microsecond=0, second=0, minute=0)\n",
    "    end_date = parser.parse(time_range[1]).replace(microsecond=0, second=0, minute=0)\n",
    "    \n",
    "    return [start_date, end_date,end_date, end_date]\n",
    "# @app.callback(\n",
    "#     Output('graph', 'figure'),\n",
    "#     Input('tables', 'value'),\n",
    "#     Input('columns', 'value'),\n",
    "#     Input('timeline', 'start_date'),\n",
    "#     Input('timeline', 'end_date')\n",
    "\n",
    "#     )\n",
    "# def update_figure(table, column, start_date, end_date):\n",
    "#     timestamp_col = drift_analysis.get_timestamp_col(table)\n",
    "#     if timestamp_col!=column:\n",
    "#         if start_date is not None:\n",
    "#             query =f\"{table}| where ['{timestamp_col}'] > datetime({start_date}) and ['{timestamp_col}'] <datetime({end_date})| summarize {column} =count({column}) by bin({timestamp_col}, 1d)\"\n",
    "#         else:\n",
    "#             query =f\"{table}| where ['{timestamp_col}'] <datetime({end_date})| summarize {column} =count({column}) by bin({timestamp_col}, 1d)\"\n",
    "#         filtered_df = drift_analysis.query(query)\n",
    "#         print(query)\n",
    "\n",
    "\n",
    "#         fig = go.Figure([go.Scatter(x=filtered_df[column].values, y=filtered_df[timestamp_col].values, name=f\"{column}\")])\n",
    "\n",
    "\n",
    "#         return fig\n",
    "\n",
    "\n",
    "app.run_server(mode='inline', debug=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7f364c9551711cd4699acda32e0312c3edab483ae246bf330de758088cecccb"
  },
  "kernelspec": {
   "display_name": "dlresearch",
   "language": "python",
   "name": "dlresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
